{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97815714",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports & helper displays\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img, title=None, size=(12,8)):\n",
    "    \"\"\"Display BGR image using matplotlib (converted to RGB).\"\"\"\n",
    "    if img is None:\n",
    "        print(\"Image is None\")\n",
    "        return\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(img_rgb)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894f4d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Keypoint detection, description, and matching\n",
    "\n",
    "def create_feature_detector():\n",
    "    \"\"\"\n",
    "    Prefer SIFT (opencv-contrib). Fallback to ORB if SIFT not available.\n",
    "    \"\"\"\n",
    "    if hasattr(cv2, \"SIFT_create\"):\n",
    "        return cv2.SIFT_create()\n",
    "    try:\n",
    "        return cv2.xfeatures2d.SIFT_create()\n",
    "    except:\n",
    "        return cv2.ORB_create(5000)\n",
    "\n",
    "\n",
    "def detect_and_compute(img, detector):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kp, desc = detector.detectAndCompute(gray, None)\n",
    "    return kp, desc\n",
    "\n",
    "\n",
    "def match_descriptors(desc1, desc2, use_l2=True, ratio=0.75):\n",
    "    \"\"\"\n",
    "    BFMatcher + Lowe ratio filter.\n",
    "    \"\"\"\n",
    "    if desc1 is None or desc2 is None:\n",
    "        return []\n",
    "\n",
    "    if use_l2:\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2)\n",
    "    else:\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "\n",
    "    matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    return good\n",
    "\n",
    "\n",
    "def estimate_homography(kp1, kp2, matches, thresh=5.0):\n",
    "    if len(matches) < 4:\n",
    "        return None, None\n",
    "\n",
    "    src = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    dst = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "\n",
    "    H, mask = cv2.findHomography(src, dst, cv2.RANSAC, thresh)\n",
    "    return H, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97535a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Warping using homography + canvas creation\n",
    "\n",
    "def warp_two_images(img1, img2, H):\n",
    "    \"\"\"\n",
    "    Warp img2 to img1 coordinate space → return large canvas + translation.\n",
    "    \"\"\"\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    # Corners of img2\n",
    "    corners2 = np.float32([[0,0],[w2,0],[w2,h2],[0,h2]]).reshape(-1,1,2)\n",
    "    warped_corners2 = cv2.perspectiveTransform(corners2, H)\n",
    "\n",
    "    # Corners of img1\n",
    "    corners1 = np.float32([[0,0],[w1,0],[w1,h1],[0,h1]]).reshape(-1,1,2)\n",
    "\n",
    "    all_corners = np.concatenate((corners1, warped_corners2), axis=0)\n",
    "\n",
    "    [xmin, ymin] = np.int32(all_corners.min(axis=0).ravel() - 5)\n",
    "    [xmax, ymax] = np.int32(all_corners.max(axis=0).ravel() + 5)\n",
    "\n",
    "    tx, ty = -xmin, -ymin\n",
    "    Ht = np.array([[1,0,tx],[0,1,ty],[0,0,1]])\n",
    "\n",
    "    size = (xmax - xmin, ymax - ymin)\n",
    "\n",
    "    pano = cv2.warpPerspective(img2, Ht @ H, size)\n",
    "    pano[ty:ty+h1, tx:tx+w1] = img1\n",
    "\n",
    "    return pano, (tx, ty), Ht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8c4be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Full Laplacian pyramid based multi-band blending\n",
    "\n",
    "def gaussian_pyramid(img, levels):\n",
    "    gp = [img.astype(np.float32)]\n",
    "    for _ in range(levels):\n",
    "        img = cv2.pyrDown(img)\n",
    "        gp.append(img.astype(np.float32))\n",
    "    return gp\n",
    "\n",
    "\n",
    "def laplacian_pyramid(gp):\n",
    "    lp = []\n",
    "    for i in range(len(gp)-1):\n",
    "        up = cv2.pyrUp(gp[i+1])\n",
    "        if up.shape[:2] != gp[i].shape[:2]:\n",
    "            up = cv2.resize(up, (gp[i].shape[1], gp[i].shape[0]))\n",
    "        lp.append(gp[i] - up)\n",
    "    lp.append(gp[-1])\n",
    "    return lp\n",
    "\n",
    "\n",
    "def reconstruct_from_laplacian(lp):\n",
    "    img = lp[-1]\n",
    "    for i in range(len(lp)-2, -1, -1):\n",
    "        img = cv2.pyrUp(img)\n",
    "        if img.shape[:2] != lp[i].shape[:2]:\n",
    "            img = cv2.resize(img, (lp[i].shape[1], lp[i].shape[0]))\n",
    "        img = img + lp[i]\n",
    "    return img\n",
    "\n",
    "\n",
    "def multiband_blend(img1, img2, mask, levels=6):\n",
    "    \"\"\"\n",
    "    mask: 0 = img2, 1 = img1\n",
    "    \"\"\"\n",
    "    # Gaussian pyramids\n",
    "    gp1 = gaussian_pyramid(img1, levels)\n",
    "    gp2 = gaussian_pyramid(img2, levels)\n",
    "    gpm = gaussian_pyramid(mask.astype(np.float32), levels)\n",
    "\n",
    "    # Laplacian pyramids\n",
    "    lp1 = laplacian_pyramid(gp1)\n",
    "    lp2 = laplacian_pyramid(gp2)\n",
    "\n",
    "    blended_pyr = []\n",
    "\n",
    "    # blend each level\n",
    "    for L1, L2, M in zip(lp1, lp2, gpm[::-1]):\n",
    "        if M.ndim == 2:\n",
    "            M = np.repeat(M[:, :, None], 3, axis=2)\n",
    "        if M.shape[:2] != L1.shape[:2]:\n",
    "            M = cv2.resize(M, (L1.shape[1], L1.shape[0]))\n",
    "        blended = L1 * M + L2 * (1 - M)\n",
    "        blended_pyr.append(blended)\n",
    "\n",
    "    result = reconstruct_from_laplacian(blended_pyr)\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e91ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Masks + final panorama stitching\n",
    "\n",
    "def create_masks(pano, t, img1, img2, H, Ht):\n",
    "    h, w = pano.shape[:2]\n",
    "    tx, ty = t\n",
    "\n",
    "    # img1 mask\n",
    "    mask1 = np.zeros((h, w), np.float32)\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    mask1[ty:ty+h1, tx:tx+w1] = 1.0\n",
    "\n",
    "    # img2 mask\n",
    "    mask2 = (cv2.warpPerspective(img2, Ht @ H, (w, h)).sum(axis=2) > 0).astype(np.float32)\n",
    "\n",
    "    # Overlap mask\n",
    "    overlap = (mask1 > 0) & (mask2 > 0)\n",
    "\n",
    "    mask = mask1.copy()\n",
    "    if overlap.any():\n",
    "        ys, xs = np.where(overlap)\n",
    "        xmin, xmax = xs.min(), xs.max()\n",
    "        width = max(1, xmax - xmin)\n",
    "\n",
    "        ramp = np.clip((xmax - np.arange(w)) / width, 0, 1)\n",
    "\n",
    "        for y in range(h):\n",
    "            if overlap[y].any():\n",
    "                mask[y] = ramp\n",
    "\n",
    "    # Smooth mask\n",
    "    mask = cv2.GaussianBlur(mask, (41,41), 15)\n",
    "    mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-6)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def stitch(img1, img2, levels=6):\n",
    "    det = create_feature_detector()\n",
    "    kp1, d1 = detect_and_compute(img1, det)\n",
    "    kp2, d2 = detect_and_compute(img2, det)\n",
    "\n",
    "    use_l2 = hasattr(cv2, \"SIFT_create\")\n",
    "    matches = match_descriptors(d1, d2, use_l2)\n",
    "\n",
    "    print(\"Matches:\", len(matches))\n",
    "\n",
    "    H, _ = estimate_homography(kp1, kp2, matches)\n",
    "    if H is None:\n",
    "        raise Exception(\"Homography failed\")\n",
    "\n",
    "    pano, t, Ht = warp_two_images(img1, img2, H)\n",
    "\n",
    "    # Prepare blending inputs\n",
    "    img1_on = np.zeros_like(pano)\n",
    "    tx, ty = t\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    img1_on[ty:ty+h1, tx:tx+w1] = img1\n",
    "\n",
    "    img2_on = cv2.warpPerspective(img2, Ht @ H, (pano.shape[1], pano.shape[0]))\n",
    "\n",
    "    mask = create_masks(pano, t, img1, img2, H, Ht)\n",
    "\n",
    "    result = multiband_blend(img1_on, img2_on, mask, levels)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd56704",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Example usage — replace with your own images\n",
    "\n",
    "imgA = cv2.imread(\"left.jpg\")     # <-- fill path\n",
    "imgB = cv2.imread(\"right.jpg\")    # <-- fill path\n",
    "\n",
    "if imgA is None or imgB is None:\n",
    "    print(\"Replace the file paths with actual images.\")\n",
    "else:\n",
    "    pano = stitch(imgA, imgB)\n",
    "    imshow(pano, \"Final Panorama\")\n",
    "    cv2.imwrite(\"panorama_output.jpg\", pano)\n",
    "    print(\"Saved panorama_output.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
